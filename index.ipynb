{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from typing import Union, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateparser\n",
    "from ipywidgets import FileUpload, widgets\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "# # workaround via specifying an invalid value first\n",
    "# %config Application.log_level='WORKAROUND'\n",
    "# # => fails, necessary on Fedora 27, ipython3 6.2.1\n",
    "# %config Application.log_level='DEBUG'\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "log = logging.getLogger()\n",
    "\n",
    "cache_dir = \"./.cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "from diskcache import Cache\n",
    "cache = Cache(os.path.join(cache_dir, \"diskcache\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% global variables\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/markus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/markus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as st\n",
    "\n",
    "def create_sources_data_frame(data=None):\n",
    "    sources = pd.DataFrame(data, columns=['file_name', 'date'])\n",
    "    sources.set_index('file_name', inplace=True)\n",
    "\n",
    "    return sources\n",
    "\n",
    "sources = create_sources_data_frame()\n",
    "all_uploaded_files = {}\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ital_stopwords = st.words('italian')\n",
    "en_stopwords = st.words('english')\n",
    "\n",
    "stopwords = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# widgets\n",
    "input_widgets = widgets.Output()\n",
    "input_details_widget = widgets.Output()\n",
    "data_widget = widgets.Output()\n",
    "\n",
    "start_input_processing_button = widgets.Button(description=\"Process input\", disabled=True)\n",
    "\n",
    "upload_data = FileUpload(accept=\".txt\", multiple=True)\n",
    "upload_stopword = FileUpload(accept=\".csv\", multiple=False)\n",
    "dates = [pd.Timestamp.now(), pd.Timestamp.now()]\n",
    "options = [(date.strftime(' %d %b %Y '), date) for date in dates]\n",
    "index = (0, len(options)-1)\n",
    "selection_range_slider = widgets.SelectionRangeSlider(\n",
    "    options=options,\n",
    "    index=index,\n",
    "    description='Dates',\n",
    "    orientation='horizontal',\n",
    "    layout={'width': '500px'},\n",
    "    continuous_update=False\n",
    ")\n",
    "selection_range_slider.disabled = True\n",
    "\n",
    "filtered_table = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def disable_input(disable: bool):\n",
    "\n",
    "    if disable:\n",
    "        selection_range_slider.disabled = True\n",
    "        upload_data.disabled = True\n",
    "        upload_stopword.disabled = True\n",
    "        start_input_processing_button.disabled = True\n",
    "    else:\n",
    "\n",
    "        if len(sources) > 0:\n",
    "            if len(stopwords) > 0:\n",
    "                start_input_processing_button.disabled = False\n",
    "            else:\n",
    "                start_input_processing_button.disabled = True\n",
    "            selection_range_slider.disabled = False\n",
    "        else:\n",
    "            selection_range_slider.disabled = True\n",
    "            start_input_processing_button.disabled = True\n",
    "\n",
    "        upload_data.disabled = False\n",
    "        upload_stopword.disabled = False\n",
    "\n",
    "# event-handling methods\n",
    "def refresh_input_details():\n",
    "\n",
    "    disable_input(False)\n",
    "\n",
    "    start_date = selection_range_slider.value[0]\n",
    "    end_date = selection_range_slider.value[1]\n",
    "\n",
    "    filtered_sources = sources[(sources['date']>=start_date) & (sources['date']<=end_date)]\n",
    "\n",
    "    input_details_widget.clear_output()\n",
    "    with input_details_widget:\n",
    "        print(f\"Selected timeframe: {start_date} - {end_date}\")\n",
    "        print(f\"Number of stopwords: {len(stopwords)}\")\n",
    "        display(filtered_sources)\n",
    "        display(start_input_processing_button)\n",
    "\n",
    "def extract_date(file_details):\n",
    "\n",
    "    file_name = file_details['metadata']['name']\n",
    "    _date = dateparser.parse(file_name.split(\"_\")[1], settings={'TIMEZONE': 'Europe/Berlin'})\n",
    "    return (file_name, _date)\n",
    "\n",
    "def prepare_data_for_file(row):\n",
    "\n",
    "    file_name = row.name\n",
    "\n",
    "    result = process_file_content(file_name)\n",
    "    result[\"file_name\"] = file_name\n",
    "    result[\"date\"] = row.date\n",
    "\n",
    "    ref = re.findall(r'(\\w+\\d+)_\\d{4}-\\d{2}-\\d{2}_', file_name)[0]\n",
    "    if (ref == 'sn85066408'):\n",
    "        pub_name = 'L\\'Italia'\n",
    "    elif (ref == '2012271201'):\n",
    "        pub_name = 'Cronaca Sovversiva'\n",
    "    else:\n",
    "        pub_name = None\n",
    "\n",
    "    result[\"ref\"] = ref\n",
    "    result[\"pub_name\"] = pub_name\n",
    "    return result\n",
    "\n",
    "@cache.memoize(typed=True, tag=\"tokenize\")\n",
    "def process_file_content(file_name):\n",
    "\n",
    "    log.debug(f\"computing: {file_name}\")\n",
    "\n",
    "    content_bytes = all_uploaded_files[file_name]['content']\n",
    "    content = ' ' + str(content_bytes, 'utf-8').replace('\\n', ' ') + ' '\n",
    "\n",
    "    tokenized = nltk.word_tokenize(content)  # TODO: language italian?\n",
    "    doc_prep = [w.lower() for w in tokenized if (w.isalpha() and len(w) > 2 )]\n",
    "    doc_prep_nonstop = [w for w in doc_prep if not w in stopwords]\n",
    "\n",
    "    result = {\"text\": content[0:20], \"tokenized\": tokenized, \"doc_prep\": doc_prep, \"doc_prep_nonstop\": doc_prep_nonstop}\n",
    "    log.debug(f\"finished computing: {file_name}\")\n",
    "    return result\n",
    "\n",
    "def get_filtered_df(start_date, end_date):\n",
    "    # we only read the files in the timeframe we are interested in\n",
    "\n",
    "    # df[\"text\"] = sources.text.astype(str)\n",
    "    # df[\"tokenized\"] = sources.tokenized.astype('object')\n",
    "    # df[\"doc_prep\"] = sources.doc_prep.astype('object')\n",
    "    # df[\"doc_prep_nonstop\"] = sources.doc_prep_nonstop.astype('object')\n",
    "\n",
    "    processed_data = sources[(sources['date']>=start_date) & (sources['date']<=end_date)].apply(lambda x: prepare_data_for_file(x), axis=1)\n",
    "    df = pd.DataFrame(processed_data.to_list(), index=processed_data.index)\n",
    "    return df\n",
    "\n",
    "    # for k, v in processed_data.items():\n",
    "    #     if not v:\n",
    "    #         continue\n",
    "    #\n",
    "    #     df.at[k, 'text'] = v['text']\n",
    "    #     df.at[k, 'tokenized'] = v['tokenized']\n",
    "    #     df.at[k, 'doc_prep'] = v['doc_prep']\n",
    "    #     df.at[k, 'doc_prep_nonstop'] = v['doc_prep_nonstop']\n",
    "    #\n",
    "    # return df\n",
    "\n",
    "\n",
    "def display_update(start_date, end_date):\n",
    "\n",
    "    df = get_filtered_df(start_date, end_date)\n",
    "    filtered_table.clear_output()\n",
    "    with filtered_table:\n",
    "        display(df.describe())\n",
    "        display(df)\n",
    "\n",
    "def date_range_change_handler(change):\n",
    "\n",
    "    disable_input(True)\n",
    "    # print(change.new)\n",
    "\n",
    "    # start_date = change.new[0]\n",
    "    # end_date = change.new[1]\n",
    "\n",
    "    refresh_input_details()\n",
    "\n",
    "\n",
    "def input_file_change_handler(change):\n",
    "\n",
    "    # upload_data.disabled = True\n",
    "    disable_input(True)\n",
    "\n",
    "    data = {}\n",
    "    for file_details in change.new.values():\n",
    "        file_id, _date = extract_date(file_details)\n",
    "        data[file_id] = _date\n",
    "        all_uploaded_files[file_id] = file_details\n",
    "\n",
    "    for file_id, _date in data.items():\n",
    "        if file_id in sources:\n",
    "            log.error(f\"Duplicate file: {file_id}\")\n",
    "            continue\n",
    "\n",
    "        sources.loc[file_id] = (_date,)\n",
    "\n",
    "    min_date = min(sources['date'])\n",
    "    max_date = max(sources['date'])\n",
    "\n",
    "    dates = pd.date_range(min_date, max_date, freq='D')\n",
    "    if len(dates) == 1:\n",
    "        dates = (dates[0], dates[0])\n",
    "    for date in dates:\n",
    "        print(date)\n",
    "        print(type(date))\n",
    "    options = [(date.strftime(' %d %b %Y '), date) for date in dates]\n",
    "\n",
    "    selection_range_slider.options = options\n",
    "    selection_range_slider.index = (0, len(options)-1)\n",
    "\n",
    "    upload_data.value.clear()\n",
    "    upload_data._counter = 0\n",
    "\n",
    "    refresh_input_details()\n",
    "\n",
    "    # upload_data.disabled = False\n",
    "\n",
    "\n",
    "def stopwords_input_change_handler(change):\n",
    "\n",
    "    disable_input(True)\n",
    "\n",
    "    stopwords.clear()\n",
    "    stopword_content = upload_stopword.data[0]\n",
    "    stopwords_string = str(stopword_content, \"utf-8\")\n",
    "\n",
    "    reader = csv.reader(stopwords_string.split(\"\\n\"))\n",
    "    for row in reader:\n",
    "        if not row or row == ['stopword']:\n",
    "            continue\n",
    "        stopwords.append(row[0])\n",
    "\n",
    "    stopwords.extend(en_stopwords)\n",
    "\n",
    "    refresh_input_details()\n",
    "\n",
    "def start_input_processing(event_source):\n",
    "\n",
    "    disable_input(True)\n",
    "\n",
    "    data_widget.clear_output()\n",
    "\n",
    "    with data_widget:\n",
    "        print(\"tokenizing input data...\")\n",
    "\n",
    "    start_date = selection_range_slider.value[0]\n",
    "    end_date = selection_range_slider.value[1]\n",
    "\n",
    "    filtered_df = get_filtered_df(start_date=start_date, end_date=end_date)\n",
    "\n",
    "    data_widget.clear_output()\n",
    "\n",
    "    with data_widget:\n",
    "\n",
    "        display(filtered_df)\n",
    "\n",
    "    disable_input(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19594efc6de3413ea167c8675506cea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-21\n",
      "<class 'datetime.date'>\n",
      "2020-12-21\n",
      "<class 'datetime.date'>\n"
     ]
    }
   ],
   "source": [
    "upload_data.observe(input_file_change_handler, names=['value'])\n",
    "upload_stopword.observe(stopwords_input_change_handler, names=['value'])\n",
    "selection_range_slider.observe(date_range_change_handler, names=['value'])\n",
    "\n",
    "with input_widgets:\n",
    "    print(\"Input data (text files)\")\n",
    "    display(upload_data)\n",
    "    print(\"Stopword (csv format)\")\n",
    "    display(upload_stopword)\n",
    "    print(\"Select corpus timeframe\")\n",
    "    display(selection_range_slider)\n",
    "\n",
    "display(input_widgets)\n",
    "refresh_input_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Input details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da42b4fe0eb467db5122bc3b580d808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(input_details_widget)\n",
    "start_input_processing_button.on_click(start_input_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c68861880441698786c1d8222cdefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for development, if the value of the upload widget is empty, we 'preload' some local data\n",
    "\n",
    "# cached_input_file = os.path.join(cache_dir, \"last_cached_input.pickle\")\n",
    "# if not upload.value:\n",
    "#     if os.path.exists(cached_input_file):\n",
    "#         with open(cached_input_file, \"rb\") as f:\n",
    "#             files = pickle.load(f)\n",
    "#     else:\n",
    "#         files = {}\n",
    "# else:\n",
    "#     files = upload.value\n",
    "#     with open(cached_input_file, \"wb\") as f:\n",
    "#         pickle.dump(files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cached_stopword_upload_file = os.path.join(cache_dir, \"last_stopword_upload.pickle\")\n",
    "# if not stopword_upload.value:\n",
    "#     if os.path.exists(cached_stopword_upload_file):\n",
    "#         with open(cached_stopword_upload_file, \"rb\") as f:\n",
    "#             stopword_content = pickle.load(f)\n",
    "#     else:\n",
    "#         stopword_content = {}\n",
    "# else:\n",
    "#     stopword_content = stopword_upload.data[0]\n",
    "#     with open(cached_stopword_upload_file, \"wb\") as f:\n",
    "#         pickle.dump(stopword_content, f)\n",
    "#\n",
    "# stopwords_string = str(stopword_content, \"utf-8\")\n",
    "# reader = csv.reader(stopwords_string.split(\"\\n\"))\n",
    "#\n",
    "# stopwords = []\n",
    "# for row in reader:\n",
    "#     if not row or row == ['stopword']:\n",
    "#         continue\n",
    "#     stopwords.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# min_date = min(sources['date'])\n",
    "# max_date = max(sources['date'])\n",
    "#\n",
    "# dates = pd.date_range(min_date, max_date, freq='D')\n",
    "#\n",
    "# options = [(date.strftime(' %d %b %Y '), date) for date in dates]\n",
    "# index = (0, len(options)-1)\n",
    "#\n",
    "# selection_range_slider = widgets.SelectionRangeSlider(\n",
    "#     options=options,\n",
    "#     index=index,\n",
    "#     description='Dates',\n",
    "#     orientation='horizontal',\n",
    "#     layout={'width': '500px'},\n",
    "#     continuous_update=False\n",
    "#\n",
    "# )\n",
    "#\n",
    "#\n",
    "#\n",
    "# selection_range_slider.observe(date_range_change_handler, names='value')\n",
    "# display(selection_range_slider)\n",
    "# display(filtered_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}