{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateparser\n",
    "from ipywidgets import FileUpload, widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# # workaround via specifying an invalid value first\n",
    "# %config Application.log_level='WORKAROUND'\n",
    "# # => fails, necessary on Fedora 27, ipython3 6.2.1\n",
    "# %config Application.log_level='DEBUG'\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "log = logging.getLogger()\n",
    "\n",
    "cache_dir = \"./.cache\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "from diskcache import Cache\n",
    "cache = Cache(os.path.join(cache_dir, \"diskcache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the text file to create the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35320f5ae5e4f67a963764caa7ea08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.txt', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload = FileUpload(accept=\".txt\", multiple=True)\n",
    "display(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for development, if the value of the upload widget is empty, we 'preload' some local data\n",
    "\n",
    "cached_input_file = os.path.join(cache_dir, \"last_cached_input.pickle\")\n",
    "if not upload.value:\n",
    "    if os.path.exists(cached_input_file):\n",
    "        with open(cached_input_file, \"rb\") as f:\n",
    "            files = pickle.load(f)\n",
    "    else:\n",
    "        files = {}\n",
    "else:\n",
    "    files = upload.value\n",
    "    with open(cached_input_file, \"wb\") as f:\n",
    "        pickle.dump(files, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the stopwords csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8994d6824544b386159916d11ba2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stopword_upload = FileUpload(accept=\".csv\", multiple=False)\n",
    "display(stopword_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cached_stopword_upload_file = os.path.join(cache_dir, \"last_stopword_upload.pickle\")\n",
    "if not stopword_upload.value:\n",
    "    if os.path.exists(cached_stopword_upload_file):\n",
    "        with open(cached_stopword_upload_file, \"rb\") as f:\n",
    "            stopword_content = pickle.load(f)\n",
    "    else:\n",
    "        stopword_content = {}\n",
    "else:\n",
    "    stopword_content = stopword_upload.data[0]\n",
    "    with open(cached_stopword_upload_file, \"wb\") as f:\n",
    "        pickle.dump(stopword_content, f)\n",
    "\n",
    "stopwords_string = str(stopword_content, \"utf-8\")\n",
    "reader = csv.reader(stopwords_string.split(\"\\n\"))\n",
    "\n",
    "stopwords = []\n",
    "for row in reader:\n",
    "    if not row or row == ['stopword']:\n",
    "        continue\n",
    "    stopwords.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract(file_details):\n",
    "\n",
    "    file_name = file_details['metadata']['name']\n",
    "    ref = re.findall(r'(\\w+\\d+)_\\d{4}-\\d{2}-\\d{2}_', file_name)[0]\n",
    "    date = dateparser.parse(file_name.split(\"_\")[1], settings={'TIMEZONE': 'Europe/Berlin'})\n",
    "    if (ref == 'sn85066408'):\n",
    "        pub_name = 'L\\'Italia'\n",
    "    elif (ref == '2012271201'):\n",
    "        pub_name = 'Cronaca Sovversiva'\n",
    "    else:\n",
    "        pub_name = None\n",
    "\n",
    "    return [file_name, date, ref, pub_name, \"\", np.nan, np.nan, np.nan]\n",
    "\n",
    "data = []\n",
    "for file_details in files.values():\n",
    "    row = extract(file_details)\n",
    "    data.append(row)\n",
    "sources = pd.DataFrame(data, columns=['file_name', 'date', 'ref', 'pub_name', \"text\", \"tokenized\", \"doc_prep\", \"doc_prep_nonstop\"])\n",
    "sources.set_index('file_name', inplace=True)\n",
    "sources[\"text\"] = sources.text.astype(str)\n",
    "sources[\"tokenized\"] = sources.tokenized.astype('object')\n",
    "sources[\"doc_prep\"] = sources.doc_prep.astype('object')\n",
    "sources[\"doc_prep_nonstop\"] = sources.doc_prep_nonstop.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/markus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/markus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as st\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ital_stopwords = st.words('italian')\n",
    "en_stopwords = st.words('english')\n",
    "stopwords.extend(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_file(row, start_date, end_date):\n",
    "\n",
    "    if row.text:\n",
    "        return None\n",
    "\n",
    "    if row.date < start_date or row.date > end_date:\n",
    "        return None\n",
    "\n",
    "    file_name = row.name\n",
    "\n",
    "    result = process_file(file_name)\n",
    "    return result\n",
    "\n",
    "@cache.memoize(typed=True, tag=\"tokenize\")\n",
    "def process_file(file_name):\n",
    "\n",
    "    log.debug(f\"computing: {file_name}\")\n",
    "\n",
    "    content_bytes = files[file_name]['content']\n",
    "    content = ' ' + str(content_bytes, 'utf-8').replace('\\n', ' ') + ' '\n",
    "\n",
    "    tokenized = nltk.word_tokenize(content)  # TODO: language italian?\n",
    "    doc_prep = [w.lower() for w in tokenized if (w.isalpha() and len(w) > 2 )]\n",
    "    doc_prep_nonstop = [w for w in doc_prep if not w in stopwords]\n",
    "\n",
    "    result = {\"text\": content[0:20], \"tokenized\": tokenized, \"doc_prep\": doc_prep, \"doc_prep_nonstop\": doc_prep_nonstop}\n",
    "    log.debug(f\"finished computing: {file_name}\")\n",
    "    return result\n",
    "\n",
    "def get_filtered_df(start_date, end_date):\n",
    "    # we only read the files in the timeframe we are interested in\n",
    "    missing_text_rows = sources.apply(lambda x: prepare_data_for_file(x, start_date, end_date), axis=1)\n",
    "\n",
    "    for k, v in missing_text_rows.items():\n",
    "        if not v:\n",
    "            continue\n",
    "\n",
    "        sources.at[k, 'text'] = v['text']\n",
    "        sources.at[k, 'tokenized'] = v['tokenized']\n",
    "        sources.at[k, 'doc_prep'] = v['doc_prep']\n",
    "        sources.at[k, 'doc_prep_nonstop'] = v['doc_prep_nonstop']\n",
    "\n",
    "    df = sources[(sources['date']>=start_date) & (sources['date']<=end_date)]\n",
    "    return df\n",
    "\n",
    "#     # print(t['tokenized'])\n",
    "#     # print(sources.loc[sources.file_name == t['file_name']])\n",
    "#     sources.loc[sources.file_name == t['file_name'], 'text'] = t['text']\n",
    "#     # sources.loc[sources.file_name == t['file_name'], 'tokenized'] = t['tokenized']\n",
    "#     # sources.loc[sources.file_name == t['file_name'], 'text'] = t['text']\n",
    "#     # sources.loc[sources.file_name == t['file_name'], 'tokenized'] = t['tokenized']\n",
    "#     # sources.loc[sources.file_name == t['file_name'], 'doc_prep'] = t['doc_prep']\n",
    "#     # sources.loc[sources.file_name == t['file_name'], 'doc_prep_nonstop'] = t['doc_prep_nonstop']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cd2e9626f04af9a9495a8b1da27a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectionRangeSlider(continuous_update=False, description='Dates', index=(0, 5808), layout=Layout(width='500pxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e524d5c681142e0902f791039053a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_date = min(sources['date'])\n",
    "max_date = max(sources['date'])\n",
    "\n",
    "dates = pd.date_range(min_date, max_date, freq='D')\n",
    "\n",
    "options = [(date.strftime(' %d %b %Y '), date) for date in dates]\n",
    "index = (0, len(options)-1)\n",
    "\n",
    "selection_range_slider = widgets.SelectionRangeSlider(\n",
    "    options=options,\n",
    "    index=index,\n",
    "    description='Dates',\n",
    "    orientation='horizontal',\n",
    "    layout={'width': '500px'},\n",
    "    continuous_update=False\n",
    "\n",
    ")\n",
    "\n",
    "filtered_table = widgets.Output()\n",
    "\n",
    "def date_range_change_handler(change):\n",
    "\n",
    "    start_date = change.new[0]\n",
    "    end_date = change.new[1]\n",
    "    df = get_filtered_df(start_date, end_date)\n",
    "    filtered_table.clear_output()\n",
    "    with filtered_table:\n",
    "        display(df.describe())\n",
    "        display(df)\n",
    "\n",
    "\n",
    "selection_range_slider.observe(date_range_change_handler, names='value')\n",
    "display(selection_range_slider)\n",
    "display(filtered_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}